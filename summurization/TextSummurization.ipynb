{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukas/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.shared import Inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "bachelor_thesis = Document('./../Linss-Lukas-ba.docx')\n",
    "paragraphs = bachelor_thesis.paragraphs\n",
    "paragraph_dict = {}\n",
    "for i, p in enumerate(paragraphs):\n",
    "    if len(p.text) > 100:\n",
    "        paragraph_dict[(paragraphs[i - 1].text, int(paragraphs[i - 1].style.name[-1]))] = p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('Abstract', 1), ('Introduction', 1), ('Objectives and Scientific Approach  ', 2), ('Literature Review', 1), ('The Metaverse', 2), ('What is the Metaverse?', 2), ('Metaverse building blocks', 2), ('Metaverse Architecture', 2), ('Metaverse Technologies', 2), ('Metaverse Platforms', 2), ('Shopping Experiences in the Metaverse', 2), ('Virtual-Commerce', 2), ('vs. e-commerce', 2), ('Influencing factors on the virtual shopping experience', 2), ('V-commerce products and spaces', 2), ('Influencing factors', 2), ('Current virtual shopping experiences', 2), ('Metaverse Development', 2), ('Building Virtual Worlds', 2), ('Virtual World Platforms & User Generated Content', 2), ('Methodology', 1), ('Context & Motivation', 2), ('The Solution', 2), ('Business Model', 2), ('Solution Development', 2), ('Development Process', 2), ('Technology Stack', 2), ('Solution Requirements', 2), ('Forms of Immersive Technology: Accessible Virtual Space', 2), ('Interface: Sense of Presence', 2), ('Interface: Scalability', 2), ('Performance: Resource Management', 2), ('Performance: Scene Performance', 2), ('Intelligence: Modular Store Design', 2), ('Function: Payment Integration', 2), ('Function: User Analytics', 2), ('Implementation', 2), ('Virtual Shopping Mall', 2), ('Mall Atmospherics and Layout', 2), ('Environmental features', 2), ('Product presentation features', 2), ('Pathfinding features', 2), ('Geo-Location', 2), ('Scene Management', 2), ('Modular Stores', 2), ('Payment Links', 2), ('Crypto ATM', 2), ('Atlas Analytics', 2), ('Analysis', 1), ('Customers', 2), ('Store configuration', 2), ('User metrics', 2), ('Discussion', 2), ('Conclusion', 1), ('Critical Appraisal', 1)])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_dict.keys()\n",
    "#print(paragraph_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import math\n",
    "\n",
    "stop_words = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summurizer(**kwargs):\n",
    "    document = kwargs.get('doc')\n",
    "    doc = nlp(document)\n",
    "\n",
    "    # Create dictionary with tokens as keys and their frequency as the value\n",
    "    token_frequencies = {}\n",
    "    for token in doc:\n",
    "        if token.text not in stop_words:\n",
    "            if token.text not in token_frequencies.keys():\n",
    "                token_frequencies[token.text] = 1\n",
    "            else:\n",
    "                token_frequencies[token.text] += 1\n",
    "    \n",
    "    # Divide each token by the maximum frequency to get the waited frequency of each token\n",
    "    for token in token_frequencies:\n",
    "        token_frequencies[token] = token_frequencies[token] / max(token_frequencies.values())\n",
    "\n",
    "    # Create dictionary with sentence as key and scentence score as value\n",
    "    scentence_list = [s for s in doc.sents]\n",
    "    scentence_score = {}\n",
    "\n",
    "    # sentence score is equal to sum of token scores for each token in the scentence\n",
    "    for sentence in scentence_list:\n",
    "        for token in sentence:\n",
    "            if token.text.lower() in token_frequencies.keys():\n",
    "                if len(sentence.text.split(' ')) < 30:\n",
    "                    if sentence not in scentence_score.keys():\n",
    "                        scentence_score[sentence] = token_frequencies[token.text.lower()]\n",
    "                    else:\n",
    "                        scentence_score[sentence] += token_frequencies[token.text.lower()]\n",
    "\n",
    "    # Summurize documents to n-sentences\n",
    "    n = math.floor(kwargs.get('ratio') * len(scentence_list))\n",
    "\n",
    "    summurized_sentences = nlargest(n, scentence_score, key=scentence_score.get)\n",
    "    summary = [s for s in scentence_score.keys() if s in summurized_sentences]\n",
    "    new_text = ' '.join([s.text for s in summary])\n",
    "    \n",
    "    print(len(document), '-', len(new_text),'=',f'{len(document) - len(new_text)} words saved')\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractKeywords(**kwargs):\n",
    "    doc = nlp(kwargs.get('doc'))\n",
    "    tags = ['PROPN', 'ADJ', 'NOUN']\n",
    "    keywords = {}\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text not in stop_words and token.text not in punctuation:\n",
    "            if token.pos_ in tags:\n",
    "                if token.text in keywords.keys():\n",
    "                    keywords[token.text] += 1\n",
    "                else:\n",
    "                    keywords[token.text] = 1\n",
    "        \n",
    "    return nlargest(kwargs.get('n'), keywords, key=keywords.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "# The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
    "nlp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentAnalysis(**kwargs):\n",
    "    doc = nlp(kwargs.get('doc'))\n",
    "    return doc._.assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['becoming'], 0.45, 0.8500000000000001, None),\n",
       " (['becoming'], 0.45, 0.8500000000000001, None),\n",
       " (['promising'], 0.2, 0.5, None),\n",
       " (['new'], 0.13636363636363635, 0.45454545454545453, None),\n",
       " (['such'], 0.0, 0.5, None),\n",
       " (['new'], 0.13636363636363635, 0.45454545454545453, None),\n",
       " (['many'], 0.5, 0.5, None),\n",
       " (['clear'], 0.10000000000000002, 0.3833333333333333, None),\n",
       " (['present'], 0.0, 0.0, None),\n",
       " (['academic'], 0.0, 0.0, None),\n",
       " (['other'], -0.125, 0.375, None),\n",
       " (['commercial'], 0.0, 0.0, None),\n",
       " (['traditional'], 0.0, 0.75, None),\n",
       " (['many'], 0.5, 0.5, None),\n",
       " (['interested'], 0.25, 0.5, None),\n",
       " (['new'], 0.13636363636363635, 0.45454545454545453, None),\n",
       " (['several'], 0.0, 0.0, None),\n",
       " (['many'], 0.5, 0.5, None),\n",
       " (['tries'], -0.1, 0.4, None),\n",
       " (['light'], 0.4, 0.7, None),\n",
       " (['following'], 0.0, 0.1, None),\n",
       " (['easy'], 0.43333333333333335, 0.8333333333333334, None),\n",
       " (['initial'], 0.0, 0.0, None),\n",
       " (['multiple'], 0.0, 0.0, None),\n",
       " (['such'], 0.0, 0.5, None),\n",
       " (['various'], 0.0, 0.5, None),\n",
       " (['atmospheric'], 0.0, 0.0, None),\n",
       " (['successfully'], 0.75, 0.95, None),\n",
       " (['able'], 0.5, 0.625, None),\n",
       " (['first'], 0.25, 0.3333333333333333, None),\n",
       " (['following'], 0.0, 0.1, None)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentAnalysis(doc=list(paragraph_dict.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virtual', 'metaverse', 'store', 'channel', 'shopping']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractKeywords(doc=list(paragraph_dict.values())[0], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 - 793 = 1219 words saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'While many brands are interested in utilising this new channel, creating them comes with several challenges and uncertainties as many companies lack experience within this field. The scope of this research included the initial requirements analysis, followed by the implementation, and launch of the solution. Throughout the development process, multiple requirements such as resource management, modular design, and payment integration were identified and implemented. Additionally, various store atmospheric elements were utilised in the design of the user interface. The resulting solution was successfully launched and was able to attract eight customers who leased a store within the mall. Furthermore, the mall was visited by 145 users within the first three weeks following the launch.\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summurizer(doc=list(paragraph_dict.values())[0], ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 - 793 = 1219 words saved\n",
      "2709 - 917 = 1792 words saved\n",
      "1719 - 789 = 930 words saved\n",
      "767 - 303 = 464 words saved\n",
      "3801 - 786 = 3015 words saved\n",
      "4893 - 1592 = 3301 words saved\n",
      "829 - 315 = 514 words saved\n",
      "2930 - 1177 = 1753 words saved\n",
      "7969 - 3855 = 4114 words saved\n",
      "3340 - 1565 = 1775 words saved\n",
      "994 - 327 = 667 words saved\n",
      "3778 - 1723 = 2055 words saved\n",
      "5181 - 2568 = 2613 words saved\n",
      "731 - 227 = 504 words saved\n",
      "1744 - 781 = 963 words saved\n",
      "8290 - 3787 = 4503 words saved\n",
      "4007 - 1754 = 2253 words saved\n",
      "484 - 168 = 316 words saved\n",
      "5009 - 2265 = 2744 words saved\n",
      "3466 - 1403 = 2063 words saved\n",
      "853 - 311 = 542 words saved\n",
      "2491 - 1136 = 1355 words saved\n",
      "4466 - 2186 = 2280 words saved\n",
      "4256 - 1820 = 2436 words saved\n",
      "534 - 260 = 274 words saved\n",
      "2534 - 1011 = 1523 words saved\n",
      "5460 - 2556 = 2904 words saved\n",
      "506 - 254 = 252 words saved\n",
      "523 - 274 = 249 words saved\n",
      "624 - 223 = 401 words saved\n",
      "967 - 388 = 579 words saved\n",
      "1261 - 483 = 778 words saved\n",
      "987 - 453 = 534 words saved\n",
      "714 - 286 = 428 words saved\n",
      "675 - 304 = 371 words saved\n",
      "655 - 308 = 347 words saved\n",
      "603 - 295 = 308 words saved\n",
      "1901 - 1078 = 823 words saved\n",
      "386 - 194 = 192 words saved\n",
      "1569 - 627 = 942 words saved\n",
      "1076 - 510 = 566 words saved\n",
      "1321 - 634 = 687 words saved\n",
      "1882 - 757 = 1125 words saved\n",
      "5261 - 2666 = 2595 words saved\n",
      "2449 - 1098 = 1351 words saved\n",
      "1026 - 431 = 595 words saved\n",
      "1324 - 625 = 699 words saved\n",
      "1743 - 1009 = 734 words saved\n",
      "560 - 224 = 336 words saved\n",
      "934 - 504 = 430 words saved\n",
      "2082 - 948 = 1134 words saved\n",
      "1193 - 547 = 646 words saved\n",
      "4253 - 1921 = 2332 words saved\n",
      "3053 - 1250 = 1803 words saved\n",
      "999 - 385 = 614 words saved\n"
     ]
    }
   ],
   "source": [
    "with open('summurized_text.txt', 'w') as t:\n",
    "\n",
    "    document = Document()\n",
    "    wc_picture = document.add_picture('./../WordCloud/word_cloud.png', width=Inches(4), height=Inches(4))\n",
    "    pic_paragraph = document.paragraphs[-1] \n",
    "    pic_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "    title = document.add_heading('Shopping Experiences in the Metaverse:', 0)\n",
    "    sub_title = document.add_heading('Implementation of a white lable Metaverse Solution', 1)\n",
    "    title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    sub_title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    document.add_page_break()\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    for k, v in paragraph_dict.items():\n",
    "    \n",
    "        document.add_heading(k[0], k[1])\n",
    "        paragraph = document.add_paragraph(summurizer(doc=v, ratio=0.5))\n",
    "        paragraph.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "        document.add_paragraph(f'Keywords: { extractKeywords(doc=v, n=4) }')\n",
    "        document.add_paragraph()\n",
    "        #document.add_page_break()\n",
    "        i += 1\n",
    "    \n",
    "    document.save('ba_summary.docx')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
