{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukas/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import math\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "summurizer = pipeline('summarization')\n",
    "stop_words = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.shared import Inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x157612640>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "# The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
    "nlp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Paragraph:\n",
    "\n",
    "    def __init__(self, content, heading, heading_style) -> None:\n",
    "        self.content = content\n",
    "        self.heading = heading\n",
    "        self.heading_style = heading_style\n",
    "        self.keywords = self.get_keywords(n=4)\n",
    "        self.summary = self.summurize(ratio=0.5)\n",
    "        self.summary_abs = self.summurize(strategy='abstract')\n",
    "        self.polarity = self.get_polarity()\n",
    "        self.subjectivity = self.get_subjectivity()\n",
    "    \n",
    "    def get_keywords(self, **kwargs):\n",
    "        doc = nlp(self.content)\n",
    "        tags = ['PROPN', 'ADJ', 'NOUN']\n",
    "        keywords = {}\n",
    "\n",
    "        for token in doc:\n",
    "            if token.text not in stop_words and token.text not in punctuation:\n",
    "                if token.pos_ in tags:\n",
    "                    if token.text in keywords.keys():\n",
    "                        keywords[token.text] += 1\n",
    "                    else:\n",
    "                        keywords[token.text] = 1\n",
    "            \n",
    "        return nlargest(kwargs.get('n'), keywords, key=keywords.get)\n",
    "    \n",
    "    def summurize(self, **kwargs):\n",
    "\n",
    "        if kwargs.get('strategy') == 'abstract':\n",
    "            try:\n",
    "                return [v for k, v  in summurizer('summarize:' + self.content)[0].items()][0]\n",
    "            except:\n",
    "                return self.summary\n",
    "\n",
    "        doc = nlp(self.content)\n",
    "\n",
    "        # Create dictionary with tokens as keys and their frequency as the value\n",
    "        token_frequencies = {}\n",
    "        for token in doc:\n",
    "            if token.text not in stop_words:\n",
    "                if token.text not in token_frequencies.keys():\n",
    "                    token_frequencies[token.text] = 1\n",
    "                else:\n",
    "                    token_frequencies[token.text] += 1\n",
    "        \n",
    "        # Divide each token by the maximum frequency to get the waited frequency of each token\n",
    "        for token in token_frequencies:\n",
    "            token_frequencies[token] = token_frequencies[token] / max(token_frequencies.values())\n",
    "\n",
    "        # Create dictionary with sentence as key and scentence score as value\n",
    "        scentence_list = [s for s in doc.sents]\n",
    "        scentence_score = {}\n",
    "\n",
    "        # sentence score is equal to sum of token scores for each token in the scentence\n",
    "        for sentence in scentence_list:\n",
    "            for token in sentence:\n",
    "                if token.text.lower() in token_frequencies.keys():\n",
    "                    if len(sentence.text.split(' ')) < 30:\n",
    "                        if sentence not in scentence_score.keys():\n",
    "                            scentence_score[sentence] = token_frequencies[token.text.lower()]\n",
    "                        else:\n",
    "                            scentence_score[sentence] += token_frequencies[token.text.lower()]\n",
    "\n",
    "        # Summurize documents to n-sentences\n",
    "        n = math.floor(kwargs.get('ratio') * len(scentence_list))\n",
    "\n",
    "        summurized_sentences = nlargest(n, scentence_score, key=scentence_score.get)\n",
    "        summary = [s for s in scentence_score.keys() if s in summurized_sentences]\n",
    "        new_text = ' '.join([s.text for s in summary])\n",
    "        \n",
    "        print(len(self.content), '-', len(new_text),'=',f'{len(self.content) - len(new_text)} words saved')\n",
    "\n",
    "        return new_text\n",
    "    \n",
    "    # a float within the range [-1.0, 1.0].\n",
    "    def get_polarity(self):\n",
    "        polarity_num = nlp(self.content)._.polarity\n",
    "        polarity_txt = ''\n",
    "        \n",
    "        if polarity_num >= 0.6:\n",
    "            polarity_txt = 'Very Positive'\n",
    "        elif polarity_num >= 0.2:\n",
    "            polarity_txt = 'Positive'\n",
    "        elif polarity_num >= -0.2:\n",
    "            polarity_txt = 'Neutral'\n",
    "        elif polarity_num >= -0.6:\n",
    "            polarity_txt = 'Negative'\n",
    "        elif polarity_num < -0.6:\n",
    "            polarity_txt = 'Very Negative'\n",
    "\n",
    "        return (polarity_txt, polarity_num)\n",
    "    # a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective. \n",
    "    def get_subjectivity(self):\n",
    "        return nlp(self.content)._.subjectivity        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bachelor_thesis = Document('./../Linss-Lukas-ba.docx')\n",
    "paragraphs = bachelor_thesis.paragraphs\n",
    "paragraph_dict = {}\n",
    "for i, p in enumerate(paragraphs):\n",
    "    if len(p.text) > 100:\n",
    "        paragraph_dict[(paragraphs[i - 1].text, int(paragraphs[i - 1].style.name[-1]))] = p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 - 793 = 1219 words saved\n",
      "2709 - 917 = 1792 words saved\n",
      "1719 - 789 = 930 words saved\n",
      "767 - 303 = 464 words saved\n",
      "3801 - 786 = 3015 words saved\n",
      "4893 - 1592 = 3301 words saved\n",
      "829 - 315 = 514 words saved\n",
      "2930 - 1177 = 1753 words saved\n",
      "7969 - 3855 = 4114 words saved\n",
      "3340 - 1565 = 1775 words saved\n",
      "994 - 327 = 667 words saved\n",
      "3778 - 1723 = 2055 words saved\n",
      "5181 - 2568 = 2613 words saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 127. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731 - 227 = 504 words saved\n",
      "1744 - 781 = 963 words saved\n",
      "8290 - 3787 = 4503 words saved\n",
      "4007 - 1754 = 2253 words saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 87. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484 - 168 = 316 words saved\n",
      "5009 - 2265 = 2744 words saved\n",
      "3466 - 1403 = 2063 words saved\n",
      "853 - 311 = 542 words saved\n",
      "2491 - 1136 = 1355 words saved\n",
      "4466 - 2186 = 2280 words saved\n",
      "4256 - 1820 = 2436 words saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 97. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534 - 260 = 274 words saved\n",
      "2534 - 1011 = 1523 words saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 97. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5460 - 2556 = 2904 words saved\n",
      "506 - 254 = 252 words saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 101. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523 - 274 = 249 words saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 127. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624 - 223 = 401 words saved\n",
      "967 - 388 = 579 words saved\n",
      "1261 - 483 = 778 words saved\n",
      "987 - 453 = 534 words saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 141. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=70)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714 - 286 = 428 words saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 121. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675 - 304 = 371 words saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 125. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655 - 308 = 347 words saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 111. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=55)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603 - 295 = 308 words saved\n",
      "1901 - 1078 = 823 words saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 70. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386 - 194 = 192 words saved\n",
      "1569 - 627 = 942 words saved\n",
      "1076 - 510 = 566 words saved\n",
      "1321 - 634 = 687 words saved\n",
      "1882 - 757 = 1125 words saved\n",
      "5261 - 2666 = 2595 words saved\n",
      "2449 - 1098 = 1351 words saved\n",
      "1026 - 431 = 595 words saved\n",
      "1324 - 625 = 699 words saved\n",
      "1743 - 1009 = 734 words saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 113. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=56)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560 - 224 = 336 words saved\n",
      "934 - 504 = 430 words saved\n",
      "2082 - 948 = 1134 words saved\n",
      "1193 - 547 = 646 words saved\n",
      "4253 - 1921 = 2332 words saved\n",
      "3053 - 1250 = 1803 words saved\n",
      "999 - 385 = 614 words saved\n"
     ]
    }
   ],
   "source": [
    "paragraph_objs = list()\n",
    "for i, p in enumerate(paragraphs):\n",
    "    if len(p.text) > 100:\n",
    "        paragraph_objs.append(Paragraph(p.text, paragraphs[i - 1].text, int(paragraphs[i - 1].style.name[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The term “metaverse” has reached mainstream audiences and has had a polarising effect on consumers and businesses alike . While consumers seem to be confused by the concept, others interpret current metaverse developments as a mere fad at best and see it as the newest marketing buzzword . While looking at various use cases, this thesis will mainly explore the metaverses' potential as a platform for creating immersive shopping and advertising experiences . This thesis will analyse the current state of this field of study and show how metaverse shopping experiences can be implemented .\""
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#paragraph_dict.keys()\n",
    "paragraph_objs[1].summury_abs\n",
    "#paragraph_objs[0].summurize(ratio=0.5)\n",
    "#print(paragraph_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' With the use of virtual worlds becoming a growing trend, the metaverse is becoming a promising channel for Brands to offer their customers immersive shopping experiences . While many brands are interested in utilising this new channel, creating them comes with several challenges and uncertainties as many companies lack experience within this field . This thesis tries to shed light on the process associated with building a virtual shopping experience by following the process of developing a white-label shopping mall solution . The resulting solution was successfully launched and was able to attract eight customers who leased a store .'}]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summurizer(paragraph_objs[0].content, min_length=100, max_length=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summurizer(**kwargs):\n",
    "    document = kwargs.get('doc')\n",
    "    doc = nlp(document)\n",
    "\n",
    "    # Create dictionary with tokens as keys and their frequency as the value\n",
    "    token_frequencies = {}\n",
    "    for token in doc:\n",
    "        if token.text not in stop_words:\n",
    "            if token.text not in token_frequencies.keys():\n",
    "                token_frequencies[token.text] = 1\n",
    "            else:\n",
    "                token_frequencies[token.text] += 1\n",
    "    \n",
    "    # Divide each token by the maximum frequency to get the waited frequency of each token\n",
    "    for token in token_frequencies:\n",
    "        token_frequencies[token] = token_frequencies[token] / max(token_frequencies.values())\n",
    "\n",
    "    # Create dictionary with sentence as key and scentence score as value\n",
    "    scentence_list = [s for s in doc.sents]\n",
    "    scentence_score = {}\n",
    "\n",
    "    # sentence score is equal to sum of token scores for each token in the scentence\n",
    "    for sentence in scentence_list:\n",
    "        for token in sentence:\n",
    "            if token.text.lower() in token_frequencies.keys():\n",
    "                if len(sentence.text.split(' ')) < 30:\n",
    "                    if sentence not in scentence_score.keys():\n",
    "                        scentence_score[sentence] = token_frequencies[token.text.lower()]\n",
    "                    else:\n",
    "                        scentence_score[sentence] += token_frequencies[token.text.lower()]\n",
    "\n",
    "    # Summurize documents to n-sentences\n",
    "    n = math.floor(kwargs.get('ratio') * len(scentence_list))\n",
    "\n",
    "    summurized_sentences = nlargest(n, scentence_score, key=scentence_score.get)\n",
    "    summary = [s for s in scentence_score.keys() if s in summurized_sentences]\n",
    "    new_text = ' '.join([s.text for s in summary])\n",
    "    \n",
    "    print(len(document), '-', len(new_text),'=',f'{len(document) - len(new_text)} words saved')\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractKeywords(**kwargs):\n",
    "    doc = nlp(kwargs.get('doc'))\n",
    "    tags = ['PROPN', 'ADJ', 'NOUN']\n",
    "    keywords = {}\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text not in stop_words and token.text not in punctuation:\n",
    "            if token.pos_ in tags:\n",
    "                if token.text in keywords.keys():\n",
    "                    keywords[token.text] += 1\n",
    "                else:\n",
    "                    keywords[token.text] = 1\n",
    "        \n",
    "    return nlargest(kwargs.get('n'), keywords, key=keywords.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentAnalysis(**kwargs):\n",
    "    doc = nlp(kwargs.get('doc'))\n",
    "    return doc._.assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virtual', 'metaverse', 'store', 'channel', 'shopping']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractKeywords(doc=list(paragraph_dict.values())[0], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summurizer(doc=list(paragraph_dict.values())[0], ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document()\n",
    "wc_picture = document.add_picture('./../WordCloud/word_cloud.png', width=Inches(4), height=Inches(4))\n",
    "pic_paragraph = document.paragraphs[-1] \n",
    "pic_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "title = document.add_heading('Shopping Experiences in the Metaverse', 0)\n",
    "sub_title = document.add_heading('Implementation of a white lable Metaverse Solution', 1)\n",
    "title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "sub_title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "document.add_page_break()\n",
    "\n",
    "\n",
    "for p in paragraph_objs:\n",
    "    document.add_heading(p.heading, p.heading_style)\n",
    "    paragraph = document.add_paragraph(p.summary_abs)\n",
    "    paragraph.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "    document.add_paragraph(f'Keywords: { p.keywords }')\n",
    "    document.add_paragraph(f'Polarity: {p.polarity[0]} ({round(p.polarity[1], 2)})')\n",
    "    document.add_paragraph(f'Subjectivity: {round(p.subjectivity, 2)} / 1.0')\n",
    "\n",
    "    document.add_page_break()\n",
    "\n",
    "document.save('ba_summary.docx')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
